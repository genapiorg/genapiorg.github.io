{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"/]+|(?!\\b)(?=[A-Z][a-z])|\\.(?!\\d)|&[lg]t;","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Easy API use for Generative AI apps","text":"<p>Building Generative AI apps is hard enough when handling LLM peculiarities like hallucinations, latency, and cost. These apps can be increasingly more useful when integrating with enterprise and third-party APIs. However, rapid and scalable API integration with LLMs is even harder. GenAPI is on a mission to enable easy API use for Generative AI apps.</p> <p></p> <p>GenAPI has five goals to make this possible:</p> <ol> <li>Reusable functions library for popular use cases like checking the weather, </li> <li>Simple helper APIs to make app prototyping as easy as running cells on a notebook,</li> <li>Quickstart cookbook with notebooks for building Generative AI Apps using best practices,</li> <li>Comprehensive documentation with API, LLM, and app design tips, and</li> <li>Efficient workflow and project structure for building scalable Generative AI Apps.</li> </ol>"},{"location":"#cookbook","title":"Cookbook","text":"<p>Our launch recipe is using OpenAI Functions with Climate APIs available as a Jupyter notebook. We use this notebook to illustrate a number of fundamental concepts. For example, we demonstrate how to use the helper API to simulate a chat experience within a Jupyter notebook with a simple statement. We also demonstrate with the accompanying Climate API examples how to write a function and crisp specification which can be consumed by the LLM with accuracy.</p>"},{"location":"tags/","title":"Tags","text":""},{"location":"tags/#functions","title":"Functions","text":"<ul> <li>OpenAI Functions with Climate API</li> </ul>"},{"location":"functions/openai-functions-with-climate-apis/","title":"OpenAI Functions with Climate APIs","text":"<p>Most popular user experience based on Large Language Models is the chat experience popularized by ChatGPT. Bard and Bing use cases of LLM follow a specialization of this chat experience in the form of questions and answers (Q&amp;A). When enterprise build apps using LLMs, they are likely to use a combination of chat and Q&amp;A experiences. These apps are more useful when they are able to integrate existing apps and services activated based on the context of the chat or Q&amp;A experience. Think of this as a conversation with your assistant about a certain topic. At some point you may suggest actions for the assistant to execute based on the context of the conversation. This is where the concept of functions come in. Functions are a way to specify actions that can be executed based on the context of the conversation.</p> <p>OpenAI Functions were introduced in July 2023 as a way for developers to have GPT 4 and GPT 3.5 models identify when to process user inputs and extract JSON arguments matching function specs specified in the chat context.</p> <p>API Tips</p> <p>You can specify a functions in a similar format as a typical function documentation describing the function name, description, arguments, and return values. GenAPI project structure recommends writing the function spec next to the function definition so that it is easy to keep both in sync with any future changes.</p>","tags":["Functions"]},{"location":"functions/openai-functions-with-climate-apis/#helper-apis-and-functions-library","title":"Helper APIs and functions library","text":"<p>GenAPI helpers include wrappers for OpenAI APIs to maintain conversation context within a multi-step chat and perform question and action in a single-step dialog. We also include helpers for simulating chat like experience within a notebook and evaluating function calling within a conversation. </p> Importing GenAPI Helper APIs and Functions Repository<pre><code>from helpers import genapi, notebook\nfrom functions import climate\n</code></pre> <p>GenAPI also provides a functions library with a number of functions that can be used to build Generative AI Apps. The functions library is organized into a number of categories like the one we are launching for climate APIs which include weather and air quality functions. You can add these functions to your project like so.</p> Adding Climate Functions Repository<pre><code>functions = climate.functions\nfunction_names = {\n    \"weather\": climate.weather,\n    \"air_quality\": climate.air_quality\n}\n</code></pre>","tags":["Functions"]},{"location":"functions/openai-functions-with-climate-apis/#initializing-conversation-context","title":"Initializing conversation context","text":"<p>You can now initialize the conversation context with a system prompt to guide the LLM how to handle functions. </p> Initializing Conversation Context<pre><code>messages = []\nmessages.append(\n  {\n    \"role\": \"system\", \n    \"content\": '''Don't make assumptions about what values to plug into \n    functions. Ask for clarification if a user request is ambiguous.'''\n  })\n</code></pre> <p>System prompts in the context of the GPT-3 API (and GPT-4 and other similar models) refer to the initial input given to the model to generate a particular kind of response. They set the context or tone for the generated text and guide the model's responses.</p> <p>LLM Tips</p> <p>To enable iterative argument filling by the LLM where the LLM asks for missing arguments, you can use the System Prompt to guide the LLM on how to handle functions.</p> <p>For example, you could use a system prompt like \"You are a helpful assistant that provides detailed and informative responses.\" This prompt sets the model in a context where it acts as a helpful assistant and aims to provide detailed answers to questions.</p> <p>Another distinction of a system prompt is that it is app developer initiated instead of user initiated prompts or LLM initiated responses. By appending the system prompt to the messages list, the app developer is able to maintain control over the conversation context, hidden from the user who would usually see their own prompts and LLM responses only.</p>","tags":["Functions"]},{"location":"functions/openai-functions-with-climate-apis/#simulating-chat-experience","title":"Simulating chat experience","text":"<p>You can now simulate a chat experience within a notebook using the <code>genapi.chat</code> helper API. This helper API takes in the messages list and the function repository and returns a list of messages with the LLM responses. Then the helper API <code>notebook.print_chat</code> can be used to print the chat experience within the notebook one cell at a time.</p> Initial user prompt<pre><code>messages.append({\"role\": \"user\", \"content\": \"What's the weather today?\"})\nmessages = genapi.chat(messages, function_names, functions)\nnotebook.print_chat(messages)\n</code></pre> <p>This returns the following LLM response. Note that the LLM response is asking for arguments to fill in the weather function. This is possible because of two reasons. First, we have specified the function spec for the weather function and which arguments are required (you will see the function spec later in this article). Second, we have specified the system prompt to guide the LLM on how to handle functions.</p> LLM response asking for argument filling<pre><code>assistant: In which city are you interested in knowing the weather? \nPlease provide the city name, state code, and country.\n</code></pre> <p>You can go on to converse with the simulated chatbot within your notebook by adding the following user prompt.</p> User prompt with arguments<pre><code>messages.append({\"role\": \"user\", \"content\": \"I live in Sunnyvale, CA.\"})\nmessages = genapi.chat(messages, function_names, functions)\nnotebook.print_chat(messages)\n</code></pre> <p>This returns the following LLM response after calling the weather function with the user provided arguments.</p> LLM response with weather information<pre><code>assistant: The current weather in Sunnyvale, CA, USA is overcast clouds \nwith a temperature of 58.64\u00b0F.\n</code></pre>","tags":["Functions"]},{"location":"functions/openai-functions-with-climate-apis/#maintaining-conversation-context","title":"Maintaining conversation context","text":"<p>Now what if the user changes their mind and wants to weather in different city or has a preference for a diffrent unit? This is where power of LLM understanding natural language and maintaining context comes together. We are also able to do this because the <code>genapi.chat</code> helper API maintains the conversation context within the messages list. You can go on to converse with the simulated chatbot within your notebook by adding the following user prompt.</p> User prompt with changes in arguments<pre><code>messages.append({\"role\": \"user\", \"content\": \"in Metric units please.\"})\nmessages = genapi.chat(messages, function_names, functions)\nnotebook.print_chat(messages)\n</code></pre> <p>This returns the following LLM response after calling the weather function with the revised units.</p> LLM response with revised units<pre><code>assistant: The current weather in Sunnyvale, CA, USA is overcast clouds with a temperature of\n14.84\u00b0C.\n</code></pre>","tags":["Functions"]},{"location":"functions/openai-functions-with-climate-apis/#switching-conversation-context","title":"Switching conversation context","text":"<p>Now what if you wanted to switch the conversation to ask about air quality. Note that we are not specifying the location although the air quality function requires this argument. LLM will pick this up from prior context.</p> User prompt with air quality<pre><code>messages.append({\"role\": \"user\", \"content\": \"and how is the air quality here.\"})\nmessages = genapi.chat(messages, function_names, functions)\nnotebook.print_chat(messages)\n</code></pre> <p>This returns the following LLM response after calling the air quality function with the user provided arguments.</p> LLM response with air quality information<pre><code>assistant: The air quality in Sunnyvale, California, USA is 43 AQI (Air Quality Index).\n</code></pre>","tags":["Functions"]},{"location":"functions/openai-functions-with-climate-apis/#evaluating-function-calling","title":"Evaluating function calling","text":"<p>We can now evaluate the entire conversation flow to inspect how the LLM filled the arguments and called the required functions. We can also compare the function calls with the resulting LLM natural language generation.</p> Evaluating function calling<pre><code>notebook.print_chat(messages, all=True)\n</code></pre> <p>This returns the following LLM response with color coded (noticable in Notebook environment) messages alternating between user and assistant. Note that all messages apart from user and system are LLM generated.</p> LLM response with entire conversation flow<pre><code>system: Don't make assumptions about what values to plug into functions.\n    Ask for clarification if a user request is ambiguous.\n\nuser: What's the weather today?\n\nassistant: In which city are you interested in knowing the weather? Please provide the city\nname, state code, and country.\n\nuser: I live in Sunnyvale, CA.\n\nfunction (weather): {\"location\": \"Sunnyvale, CA, USA\", \"temperature\": 58.64, \"units\":\n\"Fahrenheit\", \"forecast\": \"overcast clouds\"}\n\nassistant: The current weather in Sunnyvale, CA, USA is overcast clouds with a temperature of\n58.64\u00b0F.\n\nuser: in Metric units please.\n\nfunction (weather): {\"location\": \"Sunnyvale, CA, USA\", \"temperature\": 14.84, \"units\":\n\"Celsius\", \"forecast\": \"overcast clouds\"}\n\nassistant: The current weather in Sunnyvale, CA, USA is overcast clouds with a temperature of\n14.84\u00b0C.\n\nuser: and how is the air quality here.\n\nuser: and how is the air quality here.\n\nfunction (air_quality): {\"location\": \"Sunnyvale, California, USA\", \"air_quality\": \"43 AQI\"}\n\nassistant: The air quality in Sunnyvale, California, USA is 43 AQI (Air Quality Index).\n</code></pre>","tags":["Functions"]},{"location":"functions/openai-functions-with-climate-apis/#evaluating-question-and-answering","title":"Evaluating question and answering","text":"<p>What if you wanted to evaluate various functions and their responses in a single step. This way you can try different prompt variations to see how the LLM handles each scenario. GenAPI helper API has got you covered.</p> Evaluating question and answering<pre><code>act_messages = [{\"role\": \"user\", \"content\": \"What's the air quality in New York City, NY?\"}]\nprint(genapi.act(act_messages, function_names, functions))\n</code></pre> <p>This generates the following response from LLM.</p> LLM response with air quality information<pre><code>The air quality in New York City, NY is currently 10 AQI, which is considered good.\n</code></pre>","tags":["Functions"]},{"location":"functions/openai-functions-with-climate-apis/#function-specifications","title":"Function specifications","text":"<p>Now it is time to dive into the function specification itself. We start with the specification for weather function and then share the function code. Note that the LLM only needs the function specification to be able to identify the function to call.</p> <p>Let's study the specification. Few noticable things are (1) the specification resembles function documentation with name, description, arguments, and return values, (2) the specification is written in JSON format, (3) the specification is written in a way that is easy for the LLM to understand, (4) the specification descriptions are crisp, and (5) the descriptions include example values where relevant.</p> Weather function specification<pre><code>weather_spec = {\n    \"name\": \"weather\",\n    \"description\": \"Get the current weather\",\n    \"parameters\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"city\": {\n                \"type\": \"string\",\n                \"description\": \"The city, e.g. San Francisco\",\n            },\n            \"state\": {\n                \"type\": \"string\",\n                \"description\": \"The state code, e.g. CA\",\n            },\n            \"country\": {\n                \"type\": \"string\",\n                \"description\": \"The country, e.g. USA\",\n            },\n            \"units\": {\n                \"type\": \"string\", \n                \"enum\": [\"metric\", \"imperial\"],\n                \"description\": \"Units to use when reporting weather data.\",\n            },\n        },\n        \"required\": [\"city\", \"state\", \"country\"],\n    },\n}\n</code></pre> <p>Once LLM identifies the function, the developer can then call the function and return the result to the LLM. The LLM will then use the result to generate the natural language response.</p> <p>Let's note some important points which help in integrating the function with an LLM. First, the function takes in arguments in the same sequence as in the specification. Second, the required arguments match the speficication and any arguments that are not required have a default value set. Third, when calling external APIs, the function is able to gracefully handle errors and return a JSON response with an error message. Fourth, the function returns a JSON response with keys identifiable in plain English by the LLM. Fifth, the return value JSON object keys are not defined in the specification. LLM is able to convert this JSON response into natural language response without the need for a specification.</p> Weather function code<pre><code>def weather(city, state, country, units=\"imperial\"):\n    api_key = os.getenv(\"OPEN_WEATHER_MAP_KEY\")\n    base_url = \"http://api.openweathermap.org/data/2.5/weather\"\n    complete_api_link = f\"{base_url}?q={city},{state},{country}&amp;units={units}&amp;appid={api_key}\"\n\n    response = requests.get(complete_api_link)\n\n    if response.status_code != 200:\n        return json.dumps({\"error\": \"Unable to fetch weather data\"})\n\n    data = response.json()\n\n    weather_info = {\n        \"location\": f\"{city}, {state}, {country}\",\n        \"temperature\": data['main']['temp'],\n        \"units\": 'Celsius' if units == 'metric' else 'Fahrenheit',\n        \"forecast\": data['weather'][0]['description'],\n    }\n    return json.dumps(weather_info)\n</code></pre> <p>Let us now read the specification for the air quality function and then share the function code. Note that the specification is similar to the weather function specification. The only difference is that the air quality function does not have a units argument.</p> Air quality function specification<pre><code>air_quality_spec = {\n    \"name\": \"air_quality\",\n    \"description\": \"Get the current air quality\",\n    \"parameters\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"city\": {\n                \"type\": \"string\",\n                \"description\": \"The city, e.g. San Francisco\",\n            },\n            \"state\": {\n                \"type\": \"string\",\n                \"description\": \"The full form state name, e.g. California\",\n            },\n            \"country\": {\n                \"type\": \"string\",\n                \"description\": \"The country, e.g. USA\",\n            },\n        },\n        \"required\": [\"city\", \"state\", \"country\"],\n    },\n}\n</code></pre> <p>The air quality function code is similar to the weather function code. The only difference is that the air quality function does not have a units argument.</p> Air quality function code<pre><code>def air_quality(city, state, country):\n    api_key = os.getenv(\"IQAIR_KEY\")\n    base_url = 'http://api.airvisual.com/v2/city'\n    parameters = {'city': city, 'state': state, 'country': country, 'key': api_key}\n    response = requests.get(base_url, params=parameters)\n\n    if response.status_code == 200:\n        data = response.json()\n        if 'data' in data and 'current' in data['data'] and 'pollution' in data['data']['current']:\n            aqi = data['data']['current']['pollution']['aqius']\n            air_quality_info = {\n                \"location\": f\"{city}, {state}, {country}\",\n                \"air_quality\": f\"{aqi} AQI\",\n            }\n            return json.dumps(air_quality_info)\n        else:\n            raise Exception(\"Data format is not as expected, or data not available for the requested city.\")\n    else:\n        raise Exception(f\"Failed to retrieve data: {response.status_code}\")\n</code></pre> <p>Now all that is required is to add the function specifications to the function library so these can be easily imported in our notebook.</p> Adding function specifications to function library<pre><code>functions = []\nfunctions.append(weather_spec)\nfunctions.append(air_quality_spec)\n</code></pre> <p>This structure where related functions are grouped together in a functions library is a recommended best practice for building Generative AI Apps. This enables building modular LLM apps which only need to import functions based on the use case. Combining functions from multiple libraries is also as easy as importing multiple libraries and merging the imported arrays.</p>","tags":["Functions"]},{"location":"functions/openai-functions-with-climate-apis/#conclusion","title":"Conclusion","text":"<p>You can access GenAPI cookbook here OpenAI Functions with Climate APIs. You can also reuse the helper API code for genapp and notebook from GitHub code folder.</p>","tags":["Functions"]},{"location":"tags/","title":"Tags","text":""},{"location":"tags/#functions","title":"Functions","text":"<ul> <li>OpenAI Functions with Climate API</li> </ul>"}]}