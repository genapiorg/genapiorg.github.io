{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"/]+|(?!\\b)(?=[A-Z][a-z])|\\.(?!\\d)|&[lg]t;","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Easy API use for Generative AI apps","text":"<p>Building Generative AI apps is hard enough when handling LLM peculiarities like hallucinations, latency, and cost. These apps can be increasingly more useful when integrating with enterprise and third-party APIs. However, rapid and scalable API integration with LLMs is even harder. GenAPI is on a mission to enable easy API use for Generative AI apps.</p> <p></p> <p>GenAPI has five goals to make this possible:</p> <ol> <li>Reusable functions library for popular use cases like checking the weather, </li> <li>Simple helper APIs to make app prototyping as easy as running cells on a notebook,</li> <li>Quickstart cookbook with notebooks for building Generative AI Apps using best practices,</li> <li>Comprehensive documentation with API, LLM, and app design tips, and</li> <li>Efficient workflow and project structure for building scalable Generative AI Apps.</li> </ol>"},{"location":"#quickstart-cookbook","title":"Quickstart Cookbook","text":"<p>Our launch recipe is using OpenAI Functions with Climate APIs available as a Jupyter notebook. We use this notebook to illustrate a number of fundamental concepts. For example, we demonstrate how to use the helper API to simulate a chat experience within a Jupyter notebook with a simple statement. We also demonstrate with the accompanying Climate API examples how to write a function and crisp specification which can be consumed by the LLM with accuracy.</p> <p>We have since launched OpenAI Functions with Render APIs to render paintings and charts using GPT models. We have also launched Molecule Inventor with GPT4 Functions to demonstrate complex interactive visualizations in collaboration with LLM response.</p>"},{"location":"#reusable-functions","title":"Reusable Functions","text":"<p>The GenAPI functions library is a collection of reusable functions for popular use cases. These include functions for rendering paintings, charts, and 3D molecules. We also have functions for checking the weather and air quality. Check out the code in GenAPI GitHub repo for more details.</p>"},{"location":"#helper-apis","title":"Helper APIs","text":"<p>The GenAPI helper APIs are a collection of simple APIs to make app prototyping as easy as running cells on a notebook. These include APIs for rendering to a Jupyter Notebook and simulating a chat experience. Check out the code in GenAPI GitHub repo for more details.</p>"},{"location":"tags/","title":"Tags","text":""},{"location":"tags/#dalle","title":"DALL.E","text":"<ul> <li>OpenAI Functions with Render APIs</li> </ul>"},{"location":"tags/#functions","title":"Functions","text":"<ul> <li>Molecule Inventor with GPT-4 Functions</li> <li>OpenAI Functions with Climate API</li> <li>OpenAI Functions with Render APIs</li> <li>OpenAI Functions</li> </ul>"},{"location":"tags/#gpt-4","title":"GPT-4","text":"<ul> <li>Molecule Inventor with GPT-4 Functions</li> </ul>"},{"location":"tags/#generative-ai","title":"Generative AI","text":"<ul> <li>Hugging Face Gen AI Dev Setup on Mac</li> </ul>"},{"location":"tags/#guidance","title":"Guidance","text":"<ul> <li>OpenAI Functions</li> </ul>"},{"location":"tags/#hugging-face","title":"Hugging Face","text":"<ul> <li>Hugging Face Gen AI Dev Setup on Mac</li> </ul>"},{"location":"tags/#image-generation","title":"Image Generation","text":"<ul> <li>OpenAI Functions with Render APIs</li> </ul>"},{"location":"tags/#openai","title":"OpenAI","text":"<ul> <li>OpenAI Functions with Climate API</li> <li>OpenAI Functions with Render APIs</li> <li>OpenAI Functions</li> </ul>"},{"location":"functions/molecule-inventor-with-gpt4-functions/","title":"Molecule Inventor with GPT-4 Functions","text":"<p>We now introduce a new GenAPI render function for visualizing molecules using SMILES string. Simplified molecular-input line-entry system or SMILES is a specification in the form of a line notation for describing the structure of chemical species using short ASCII strings. SMILES strings can be imported by most molecule editors for conversion back into two-dimensional drawings or three-dimensional models of the molecules.</p> <p>This function requires GPT-4 to work as GPT-3.5 does not generate SMILES strings. Let us start by writing the function specification and definition.</p> Molecule API spec<pre><code>molecule_spec = {\n    \"name\": \"molecule\",\n    \"description\": \"Render a molecule based on \n    a Simplified molecular-input line-entry system or SMILES string as an argument.\",\n    \"parameters\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"smiles\": {\n                \"type\": \"string\",\n                \"description\": \"Valid Simplified molecular-input line-entry system or SMILES string.\",\n            },\n        },\n        \"required\": [\"smiles\"],\n    },\n}\n</code></pre> <p>Our Molecule API takes a single argument called \"smiles,\" which is expected to be a SMILES (Simplified Molecular Input Line Entry System) string representing a chemical compound. The purpose of this function is to render and display a 3D molecular structure visualization of the input compound using the py3Dmol library.</p> Molecule API definition<pre><code>def molecule(smiles):\n    viewer = py3Dmol.view(width=300, height=300)\n    mol_block = Chem.MolToMolBlock(Chem.MolFromSmiles(smiles))\n    viewer.addModel(mol_block, format='sdf')\n    viewer.setStyle({'stick':{}, 'sphere':{'radius':0.5}})\n    viewer.zoomTo()    \n    viewer.show()\n    molecule_info = {\n        \"smiles\": smiles,\n        \"success\": \"Molecule rendered by function\",\n    }\n    return json.dumps(molecule_info)\n</code></pre> <p>Now let's head to the cookbook here to learn how to use this function. Our function library is growing so we selectively add only molecule function to the conversation context. This results in fewer tokens to process by GPT-4 and hence faster response time and lower cost.</p> Select molecule function from library<pre><code>functions = render.functions\n\nfunction_names = {\n    \"molecule\": render.molecule,\n}\n</code></pre> <p>Now let's set the model to GPT-4 using <code>genapi.GPT_MODEL = \"gpt-4-turbo-0613\"</code> and start the conversation.</p> Start conversation<pre><code>messages.append({\"role\": \"user\", \n                 \"content\": '''Invent a new molecule which makes a material solid at room temperature.'''})\nmessages = genapi.chat(messages, function_names, functions)\nnotebook.print_chat(messages)\n</code></pre> <p>This results in visualization of the generated SMILES string like so.</p> <p></p> <p>Now we will continue the conversation and iterate changes to our molecule.</p> Continue conversation<pre><code>messages.append({\"role\": \"user\", \n                 \"content\": '''Change the molecule to make this material bouncy. Render it.'''})\nmessages = genapi.chat(messages, function_names, functions)\nnotebook.print_chat(messages)\n</code></pre> <p>This results in visualization of the generated SMILES string like so.</p> <p></p> <p>This demonstrates how to introduce GenAPI render functions for complex visualizations like inventing a molecule in collaboration with GPT-4. If you like to see more render functions, please share this article on LinkedIn, star our GitHub repo, and let us know what you would like to see next.</p>","tags":["GPT-4","Functions"]},{"location":"functions/openai-functions-with-climate-apis/","title":"OpenAI Functions with Climate APIs","text":"<p>In this article we introduce GenAPI helper APIs and functions library to build Generative AI Apps using OpenAI Functions. We will use the helper APIs and functions library to build a notebook that simulates a chat experience with a user and an LLM. We will also evaluate the function calling and question answering capabilities of the LLM.</p>","tags":["Functions","OpenAI"]},{"location":"functions/openai-functions-with-climate-apis/#helper-apis-and-functions-library","title":"Helper APIs and functions library","text":"<p>GenAPI helpers include wrappers for OpenAI APIs to maintain conversation context within a multi-step chat and perform question and action in a single-step dialog. We also include helpers for simulating chat like experience within a notebook and evaluating function calling within a conversation. </p> Importing GenAPI Helper APIs and Functions Library<pre><code>from helpers import genapi, notebook\nfrom functions import climate\n</code></pre> <p>GenAPI also provides a functions library with a number of functions that can be used to build Generative AI Apps. The functions library is organized into a number of categories like the one we are launching for climate APIs which include weather and air quality functions. You can add these functions to your project like so.</p> Adding Climate Functions Library<pre><code>functions = climate.functions\nfunction_names = {\n    \"weather\": climate.weather,\n    \"air_quality\": climate.air_quality\n}\n</code></pre>","tags":["Functions","OpenAI"]},{"location":"functions/openai-functions-with-climate-apis/#initializing-conversation-context","title":"Initializing conversation context","text":"<p>You can now initialize the conversation context with a system prompt to guide the LLM how to handle functions. </p> Initializing Conversation Context<pre><code>messages = []\nmessages.append(\n  {\n    \"role\": \"system\", \n    \"content\": '''Don't make assumptions about what values to plug into \n    functions. Ask for clarification if a user request is ambiguous.'''\n  })\n</code></pre> <p>System prompts in the context of the GPT-3 API (and GPT-4 and other similar models) refer to the initial input given to the model to generate a particular kind of response. They set the context or tone for the generated text and guide the model's responses.</p> <p>LLM Tips</p> <p>To enable iterative argument filling by the LLM where the LLM asks for missing arguments, you can use the System Prompt to guide the LLM on how to handle functions.</p> <p>For example, you could use a system prompt like \"You are a helpful assistant that provides detailed and informative responses.\" This prompt sets the model in a context where it acts as a helpful assistant and aims to provide detailed answers to questions.</p> <p>Another distinction of a system prompt is that it is app developer initiated instead of user initiated prompts or LLM initiated responses. By appending the system prompt to the messages list, the app developer is able to maintain control over the conversation context, hidden from the user who would usually see their own prompts and LLM responses only.</p>","tags":["Functions","OpenAI"]},{"location":"functions/openai-functions-with-climate-apis/#simulating-chat-experience","title":"Simulating chat experience","text":"<p>You can now simulate a chat experience within a notebook using the <code>genapi.chat</code> helper API. This helper API takes in the messages list and the function library and returns a list of messages with the LLM responses. Then the helper API <code>notebook.print_chat</code> can be used to print the chat experience within the notebook one cell at a time.</p> Initial user prompt<pre><code>messages.append({\"role\": \"user\", \"content\": \"What's the weather today?\"})\nmessages = genapi.chat(messages, function_names, functions)\nnotebook.print_chat(messages)\n</code></pre> <p>This returns the following LLM response. Note that the LLM response is asking for arguments to fill in the weather function. This is possible because of two reasons. First, we have specified the function spec for the weather function and which arguments are required (you will see the function spec later in this article). Second, we have specified the system prompt to guide the LLM on how to handle functions.</p> LLM response asking for argument filling<pre><code>assistant: In which city are you interested in knowing the weather? \nPlease provide the city name, state code, and country.\n</code></pre> <p>You can go on to converse with the simulated chatbot within your notebook by adding the following user prompt.</p> User prompt with arguments<pre><code>messages.append({\"role\": \"user\", \"content\": \"I live in Sunnyvale, CA.\"})\nmessages = genapi.chat(messages, function_names, functions)\nnotebook.print_chat(messages)\n</code></pre> <p>This returns the following LLM response after calling the weather function with the user provided arguments.</p> LLM response with weather information<pre><code>assistant: The current weather in Sunnyvale, CA, USA is overcast clouds \nwith a temperature of 58.64\u00b0F.\n</code></pre>","tags":["Functions","OpenAI"]},{"location":"functions/openai-functions-with-climate-apis/#maintaining-conversation-context","title":"Maintaining conversation context","text":"<p>Now what if the user changes their mind and wants to weather in different city or has a preference for a diffrent unit? This is where power of LLM understanding natural language and maintaining context comes together. We are also able to do this because the <code>genapi.chat</code> helper API maintains the conversation context within the messages list. You can go on to converse with the simulated chatbot within your notebook by adding the following user prompt.</p> User prompt with changes in arguments<pre><code>messages.append({\"role\": \"user\", \"content\": \"in Metric units please.\"})\nmessages = genapi.chat(messages, function_names, functions)\nnotebook.print_chat(messages)\n</code></pre> <p>This returns the following LLM response after calling the weather function with the revised units.</p> LLM response with revised units<pre><code>assistant: The current weather in Sunnyvale, CA, USA is overcast clouds with a temperature of\n14.84\u00b0C.\n</code></pre>","tags":["Functions","OpenAI"]},{"location":"functions/openai-functions-with-climate-apis/#switching-conversation-context","title":"Switching conversation context","text":"<p>Now what if you wanted to switch the conversation to ask about air quality. Note that we are not specifying the location although the air quality function requires this argument. LLM will pick this up from prior context.</p> User prompt with air quality<pre><code>messages.append({\"role\": \"user\", \"content\": \"and how is the air quality here.\"})\nmessages = genapi.chat(messages, function_names, functions)\nnotebook.print_chat(messages)\n</code></pre> <p>This returns the following LLM response after calling the air quality function with the user provided arguments.</p> LLM response with air quality information<pre><code>assistant: The air quality in Sunnyvale, California, USA is 43 AQI (Air Quality Index).\n</code></pre>","tags":["Functions","OpenAI"]},{"location":"functions/openai-functions-with-climate-apis/#multiple-functions-in-a-single-step","title":"Multiple functions in a single step","text":"<p>What if the user wanted to ask about weather and air quality in a single step. This is where the <code>genapi.chat</code> helper API shines in utilizing the power of OpenAI LLMs. The helper API handles recursive function calling and argument filling to enable multiple functions in a single step.</p> User prompt with weather and air quality<pre><code>messages.append({\"role\": \"user\", \n    \"content\": \"How is the weather and air quality in Boston, MA?\"})\nmessages = genapi.chat(messages, function_names, functions)\nnotebook.print_chat(messages)\n</code></pre> <p>This returns the following LLM response with weather and air quality information.</p> LLM response with weather and air quality information<pre><code>assistant: The current weather in Boston, MA is scattered clouds with a \ntemperature of 47.7 degrees Fahrenheit. The air quality in Boston is 16 AQI.\n</code></pre>","tags":["Functions","OpenAI"]},{"location":"functions/openai-functions-with-climate-apis/#evaluating-function-calling","title":"Evaluating function calling","text":"<p>We can now evaluate the entire conversation flow to inspect how the LLM filled the arguments and called the required functions. We can also compare the function calls with the resulting LLM natural language generation.</p> Evaluating function calling<pre><code>notebook.print_chat(messages, all=True)\n</code></pre> <p>This returns the following LLM response with color coded messages alternating between user and assistant. Note that all messages apart from user and system are LLM generated.</p> <p></p>","tags":["Functions","OpenAI"]},{"location":"functions/openai-functions-with-climate-apis/#evaluating-question-and-answering","title":"Evaluating question and answering","text":"<p>What if you wanted to evaluate various functions and their responses in a single step. This way you can try different prompt variations to see how the LLM handles each scenario. GenAPI helper API has got you covered.</p> Evaluating question and answering<pre><code>act_messages = [{\"role\": \"user\", \n    \"content\": \"What's the air quality in New York City, NY?\"}]\nprint(genapi.act(act_messages, function_names, functions))\n</code></pre> <p>This generates the following response from LLM.</p> LLM response with air quality information<pre><code>The air quality in New York City, NY is currently 10 AQI, \nwhich is considered good.\n</code></pre>","tags":["Functions","OpenAI"]},{"location":"functions/openai-functions-with-climate-apis/#function-specifications-guidance","title":"Function specifications guidance","text":"<p>Now it is time to dive into the function specification itself. We start with the specification for weather function and then share the function code. Note that the LLM only needs the function specification to be able to identify the function to call.</p> <p>Let's study the specification. Few noticable things are (1) the specification resembles function documentation with name, description, arguments, and return values, (2) the specification is written in JSON format, (3) the specification is written in a way that is easy for the LLM to understand, (4) the specification descriptions are crisp, and (5) the descriptions include example values where relevant.</p> Weather function specification<pre><code>weather_spec = {\n    \"name\": \"weather\",\n    \"description\": \"Get the current weather\",\n    \"parameters\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"city\": {\n                \"type\": \"string\",\n                \"description\": \"The city, e.g. San Francisco\",\n            },\n            \"state\": {\n                \"type\": \"string\",\n                \"description\": \"The state code, e.g. CA\",\n            },\n            \"country\": {\n                \"type\": \"string\",\n                \"description\": \"The country, e.g. USA\",\n            },\n            \"units\": {\n                \"type\": \"string\", \n                \"enum\": [\"metric\", \"imperial\"],\n                \"description\": \"Units to use when reporting weather data.\",\n            },\n        },\n        \"required\": [\"city\", \"state\", \"country\"],\n    },\n}\n</code></pre>","tags":["Functions","OpenAI"]},{"location":"functions/openai-functions-with-climate-apis/#function-definition-guidance","title":"Function definition guidance","text":"<p>Once LLM identifies the function, the developer can then call the function and return the result to the LLM. The LLM will then use the result to generate the natural language response.</p> <p>Let's note some important points which help in integrating the function with an LLM. First, the function takes in arguments in the same sequence as in the specification. Second, the required arguments match the speficication and any arguments that are not required have a default value set. Third, when calling external APIs, the function is able to gracefully handle errors and return a JSON response with an error message. Fourth, the function returns a JSON response with keys identifiable in plain English by the LLM. Fifth, the return value JSON object keys are not defined in the specification. LLM is able to convert this JSON response into natural language response without the need for a specification.</p> Weather function code<pre><code>def weather(city, state, country, units=\"imperial\"):\n    api_key = os.getenv(\"OPEN_WEATHER_MAP_KEY\")\n    base_url = \"http://api.openweathermap.org/data/2.5/weather\"\n    complete_api_link = f\"{base_url}?q={city},{state},{country}&amp;units={units}&amp;appid={api_key}\"\n\n    response = requests.get(complete_api_link)\n\n    if response.status_code != 200:\n        return json.dumps({\"error\": \"Unable to fetch weather data\"})\n\n    data = response.json()\n\n    weather_info = {\n        \"location\": f\"{city}, {state}, {country}\",\n        \"temperature\": data['main']['temp'],\n        \"units\": 'Celsius' if units == 'metric' else 'Fahrenheit',\n        \"forecast\": data['weather'][0]['description'],\n    }\n    return json.dumps(weather_info)\n</code></pre>","tags":["Functions","OpenAI"]},{"location":"functions/openai-functions-with-climate-apis/#air-quality-function","title":"Air quality function","text":"<p>Let us now read the specification for the air quality function and then share the function code. Note that the specification is similar to the weather function specification. The only difference is that the air quality function does not have a units argument.</p> Air quality function specification<pre><code>air_quality_spec = {\n    \"name\": \"air_quality\",\n    \"description\": \"Get the current air quality\",\n    \"parameters\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"city\": {\n                \"type\": \"string\",\n                \"description\": \"The city, e.g. San Francisco\",\n            },\n            \"state\": {\n                \"type\": \"string\",\n                \"description\": \"The full form state name, e.g. California\",\n            },\n            \"country\": {\n                \"type\": \"string\",\n                \"description\": \"The country, e.g. USA\",\n            },\n        },\n        \"required\": [\"city\", \"state\", \"country\"],\n    },\n}\n</code></pre> <p>The air quality function code is similar to the weather function code. The only difference is that the air quality function does not have a units argument.</p> Air quality function code<pre><code>def air_quality(city, state, country):\n    api_key = os.getenv(\"IQAIR_KEY\")\n    base_url = 'http://api.airvisual.com/v2/city'\n    parameters = {'city': city, 'state': state, \n        'country': country, 'key': api_key}\n    response = requests.get(base_url, params=parameters)\n\n    if response.status_code == 200:\n        data = response.json()\n        if 'data' in data and 'current' in data['data'] and 'pollution' in data['data']['current']:\n            aqi = data['data']['current']['pollution']['aqius']\n            air_quality_info = {\n                \"location\": f\"{city}, {state}, {country}\",\n                \"air_quality\": f\"{aqi} AQI\",\n            }\n            return json.dumps(air_quality_info)\n        else:\n            raise Exception(\"Data format is not as expected, or data not available for the requested city.\")\n    else:\n        raise Exception(f\"Failed to retrieve data: {response.status_code}\")\n</code></pre>","tags":["Functions","OpenAI"]},{"location":"functions/openai-functions-with-climate-apis/#function-library-structure","title":"Function library structure","text":"<p>Now all that is required is to add the function specifications to the function library so these can be easily imported in our notebook.</p> Adding function specifications to function library<pre><code>functions = []\nfunctions.append(weather_spec)\nfunctions.append(air_quality_spec)\n</code></pre> <p>This structure where related functions are grouped together in a functions library is a recommended best practice for building Generative AI Apps. This enables building modular LLM apps which only need to import functions based on the use case. Combining functions from multiple libraries is also as easy as importing multiple libraries and merging the imported arrays.</p>","tags":["Functions","OpenAI"]},{"location":"functions/openai-functions-with-climate-apis/#conclusion","title":"Conclusion","text":"<p>You can access GenAPI cookbook here OpenAI Functions with Climate APIs. You can also reuse the helper API code for genapp and notebook from GitHub code/helpers folder. The functions library is available in the GitHub code/functions folder where you can study the <code>climate.py</code> file for the functions used in this article.</p>","tags":["Functions","OpenAI"]},{"location":"functions/openai-functions-with-render-apis/","title":"OpenAI Functions with Render APIs","text":"<p>This article will show you how to use GenAPI Render APIs as OpenAI Functions. Render APIs do just that, they visually render an output based on the input arguments. This is useful for generating images, videos, and charts among other applications. Designing these APIs has been both a rewarding and challenging experiment for us. </p> <p>First, we purposely selected an API to render paintings which conflicts with the recent OpenAI parametric LLM capabilities to generate images. So, in this case we are trying to override the LLM response based on API response. </p> <p>Second, we chose an API to render a chart or plot based on LLM response in the same completion. We also try to render both the API response and the LLM response together! We wanted to avoid any post processing of LLM response to render the conversational CX. This reduces the code we need to write and increases our reliance on the right prompt engineering. The hope is that these constraints will cover a wide range of applications for our users.</p>","tags":["Functions","OpenAI","DALL.E","Image Generation"]},{"location":"functions/openai-functions-with-render-apis/#overriding-llm-response-with-api-response","title":"Overriding LLM response with API response","text":"<p>Let us start by creating the render API to generate a painting based on given user inputs on painting subject, background, etc. We tried to design our API will a lot of flexibility in terms of the input parameters. At the same time our required arguments are minimal to keep prompt variations from simple to more involved.</p> Painting API spec<pre><code>painting_spec = {\n    \"name\": \"painting\",\n    \"description\": \"Generate a painting.\",\n    \"parameters\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"subject\": {\n                \"type\": \"string\",\n                \"description\": \"Subject of the painting, e.g. astronaut.\",\n            },\n            \"setting\": {\n                \"type\": \"string\",\n                \"description\": \"Background or backdrop for the painting, e.g. alien landscape.\",\n            },\n            \"medium\": {\n                \"type\": \"string\",\n                \"description\": \"Medium used for the painting, e.g. oil.\",\n            },\n            \"surface\": {\n                \"type\": \"string\",\n                \"description\": \"Surface used for the painting, e.g. canvas.\",\n            },\n            \"artist\": {\n                \"type\": \"string\",\n                \"description\": \"A well known artist, e.g. Picasso.\",\n            },\n            \"size\": {\n                \"type\": \"string\",\n                \"description\": \"Size of image to generate in pixels heightxwidth format, e.g. 1024x512\",\n            },\n        },\n        \"required\": [\"subject\", \"background\"],\n    },\n}\n</code></pre> <p>Next we define the API function to generate the painting. We use the OpenAI Image completion API for getting the results. Note that in the return JSON value we indicate <code>success</code> key value as <code>Share this image link</code>. We are hoping this instructs the LLM to not generate an image but instead use the API response. We will see how this works late in this article.</p> Painting API function<pre><code>def painting(subject, background, medium=\"oil\", surface=\"canvas\", artist=\"picasso\", size='512x512'):\n    prompt = f\"Painting of {subject} in a {background} setting, painted in {medium} on {surface} by {artist}.\"\n    response = openai.Image.create(\n            prompt=prompt,\n            n=1,\n            size=size\n        )\n    image_url = response['data'][0]['url']\n    display(Image(url=image_url))\n\n    painting_info = {\n        \"prompt\": prompt,\n        \"success\": \"Share this image link\",\n        \"image_url\": image_url,\n    }\n    return json.dumps(painting_info)\n</code></pre>","tags":["Functions","OpenAI","DALL.E","Image Generation"]},{"location":"functions/openai-functions-with-render-apis/#combining-llm-and-api-responses","title":"Combining LLM and API responses","text":"<p>Now we define the API function to generate a chart based on the LLM response as a Markdown table. We can choose to render a bar, line, or point chart.</p> Chart API spec<pre><code>table_chart_spec = {\n    \"name\": \"table_chart\",\n    \"description\": \"Render a chart based on a give Markdown table.\",\n    \"parameters\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"markdown_table\": {\n                \"type\": \"string\",\n                \"description\": \"Markdown table of data to render as a chart.\",\n            },\n            \"chart_type\": {\n                \"type\": \"string\", \n                \"enum\": [\"point\", \"bar\", \"line\"],\n                \"description\": \"Type of chart to render.\",\n            },\n        },\n        \"required\": [\"markdown_table\", \"chart_type\"],\n    },\n}\n</code></pre> <p>Next we define the API function to generate the chart. This requires a utility function <code>is_alphanumeric</code> to check column values and identify X or Y-axis columns. The API definition is somewhat complex compared to others so far as we want the API to be generic to handle any Markdown table generated by the LLM. First part of the code converts the Markdown table to a pandas DataFrame. The second part of the function plots our chart based on the DataFrame contents.</p> Chart API function<pre><code>def is_alphanumeric(s):\n    return s.isalnum() and (not s.isnumeric())\n\ndef table_chart(markdown_table, chart_type=\"bar\"):\n    df = pd.read_table(StringIO(markdown_table), sep=\"|\").dropna(axis=1, how=\"all\")\n    df.columns = df.columns.str.strip()\n    df = df.applymap(str.strip)\n    df = df.iloc[1:].reset_index(drop=True)\n\n    for col in df.columns:\n        if df[col].apply(is_alphanumeric).all():\n            continue\n\n        try:\n            df[col] = pd.to_numeric(df[col])\n        except ValueError:\n            pass\n\n    if 'Rank' in df.columns:\n        df = df.drop(columns=['Rank'])\n\n    for col in df.columns:\n        try:\n            df[col] = pd.to_numeric(df[col])\n        except ValueError:\n            pass\n\n    string_columns = df.select_dtypes(include=['object']).columns.tolist()\n    numeric_columns = df.select_dtypes(exclude=['object']).columns.tolist()\n\n    if not string_columns or not numeric_columns:\n        raise ValueError(\"Table must contain at least one string and one numeric column\")\n\n    string_column = string_columns[0]\n    numeric_column = numeric_columns[0]\n\n    if chart_type == 'bar':\n        sns.barplot(x=numeric_column, y=string_column, hue=string_column, data=df, dodge=False, legend=False)\n        plt.legend([],[], frameon=False)\n    elif chart_type == 'line':\n        sns.lineplot(x=numeric_column, y=string_column, hue=string_column, data=df, legend=False)\n        plt.legend([],[], frameon=False)\n    elif chart_type == 'point':\n        sns.scatterplot(x=numeric_column, y=string_column, hue=string_column, data=df, legend=False)\n        plt.legend([],[], frameon=False)\n    else:\n        raise ValueError(\"Unsupported chart type\")\n\n    plt.show()\n\n    table_chart_info = {\n        \"success\": \"Rendered table as a chart.\",\n        \"table\": markdown_table,\n    }\n    return json.dumps(table_chart_info)\n</code></pre> <p>All that remains is for us to add these functions to our library like so.</p> Add functions to library<pre><code>functions = []\nfunctions.append(painting_spec)\nfunctions.append(table_chart_spec)\n</code></pre>","tags":["Functions","OpenAI","DALL.E","Image Generation"]},{"location":"functions/openai-functions-with-render-apis/#rendering-the-customer-experience","title":"Rendering the customer experience","text":"<p>Now let's head to our Jupyter Notebook and try these functions out. We start by importing our helper APIs and functions library.</p> Import helper APIs and functions library<pre><code>from helpers import genapi, notebook\nfrom functions import climate, render\n</code></pre> <p>Next we combine the two libraries like so. This technique is modular so as we add more libraries we can combine them all together or decide to drop the ones we are not using for our current use case.</p> Combine libraries<pre><code>functions = climate.functions\nfunctions.extend(render.functions)\n\nfunction_names = {\n    \"weather\": climate.weather,\n    \"air_quality\": climate.air_quality,\n    \"painting\": render.painting,\n    \"table_chart\": render.table_chart\n}\n</code></pre> <p>Now we modify the system prompt to handle a few more specifics where LLM may try to take over the response generation or as we noted when calling the chart API the LLM goes into a loop and calls the function many times!</p> Modify system prompt<pre><code>messages = []\nmessages.append(\n    {\n        \"role\": \"system\", \n        \"content\": '''Ask the user for arguments when calling a function. \n        Respond after understanding function response and user intent, if they expect,\n        function response only or\n        function response and your response.\n        Don't call the same function more than once for each user prompt.\n        Remain crisp and to the point in your responses.'''\n    })\n</code></pre>","tags":["Functions","OpenAI","DALL.E","Image Generation"]},{"location":"functions/openai-functions-with-render-apis/#trying-out-chart-api","title":"Trying out Chart API","text":"<p>Now let's try the Chart API.</p> Try Chart API<pre><code>messages.append({\"role\": \"user\", \n                 \"content\": '''Generate a markdown table of 5 tallest buildings in the world.\n                 Visualize this table as a bar chart.'''})\nmessages = genapi.chat(messages, function_names, functions)\nnotebook.print_chat(messages)\n</code></pre> <p>This magically generates the following chart and markdown table combining the LLM response and the API response. Super cool isn't it?</p> <p></p>","tags":["Functions","OpenAI","DALL.E","Image Generation"]},{"location":"functions/openai-functions-with-render-apis/#testing-the-painting-api","title":"Testing the Painting API","text":"<p>Well let's try the Painting API now.</p> Try Painting API<pre><code>messages.append({\"role\": \"user\", \"content\": \"Can you help me create a painting?\"})\nmessages = genapi.chat(messages, function_names, functions)\nnotebook.print_chat(messages)\n</code></pre> <p>LLM responds by asking the user for the painting subject, background, etc.</p> LLM response<pre><code>Of course! I'd be happy to help you create a painting. Please provide me with some details:\n\nSubject of the painting (e.g., landscape, portrait, still life, etc.).\nBackground or backdrop for the painting (e.g., beach, forest, cityscape, etc.).\nMedium you would like to use for the painting (e.g., oil, acrylic, watercolor, etc.).\nSurface you would like to paint on (e.g., canvas, paper, wood, etc.).\nIf you have any preference for a well-known artist or any specific style, please let me know.\nIf you have a preferred size for the painting (in pixels height x width format), please provide that as well.\nOnce I have these details, I can generate a painting for you.\n</code></pre> <p>The user just responds with the painting subject and background.</p> User response<pre><code>messages.append({\"role\": \"user\", \n\"content\": \"it should be about an astronaut looking over a beautiful alien valley background.\"})\nmessages = genapi.chat(messages, function_names, functions)\nnotebook.print_chat(messages)\n</code></pre> <p>The response is a beautiful painting with default argument values from combined with the user provided arguments. The LLM also tries to render the image but we have instructed it to use the API response instead. This is not perfect yet and it seems we may require some post processing of LLM + API response for production customer experiences to control this.</p> <p></p> <p>We are on a roll! Let's try another prompt, this time we provide the painting subject, background, style, surface, and medium.</p> User provides many arguments<pre><code>messages.append({\"role\": \"user\", \n\"content\": '''Create painting of a lone warrier \non the streets of a futuristic cyberpunk city painted in the style of Richard Estes,\nin watercolor on sanded gessobord.'''})\nmessages = genapi.chat(messages, function_names, functions)\nnotebook.print_chat(messages)\n</code></pre> <p>Let's look at the results. The LLM responds with the completed prompt and the API response is a beautiful painting matching our arguments.</p> <p></p> LLM response<pre><code>The painting features a lone warrior on the streets of a futuristic cyberpunk city, painted in the style of Richard Estes. It is created using watercolor on sanded gessobord.\n</code></pre>","tags":["Functions","OpenAI","DALL.E","Image Generation"]},{"location":"functions/openai-functions-with-render-apis/#testing-the-weather-api","title":"Testing the Weather API","text":"<p>To test if our new APIs coexist with the existing ones we try the weather API.</p> Try weather API<pre><code>messages.append({\"role\": \"user\", \"content\": \"How is the weather in Boston, MA?\"})\nmessages = genapi.chat(messages, function_names, functions)\nnotebook.print_chat(messages)\n</code></pre> <p>And the LLM responds with <code>The current weather in Boston, MA is few clouds with a temperature of 54.25\u00b0F.</code> as expected.</p>","tags":["Functions","OpenAI","DALL.E","Image Generation"]},{"location":"functions/openai-functions-with-render-apis/#evaluating-the-conversation-session","title":"Evaluating the conversation session","text":"<p>We can also evaluate the entire conversation session using <code>notebook.print_chat(messages, all=True)</code> helper API.</p> <p></p> <p></p>","tags":["Functions","OpenAI","DALL.E","Image Generation"]},{"location":"functions/openai-functions-with-render-apis/#conclusion","title":"Conclusion","text":"<p>You can download the Jupyter Notebook for this article from here from GitHub. You can read the render APIs here.</p>","tags":["Functions","OpenAI","DALL.E","Image Generation"]},{"location":"functions/openai-functions/","title":"OpenAI Functions","text":"<p>Most popular user experience based on Large Language Models is the chat experience popularized by ChatGPT. Bard and Bing use cases of LLM follow a specialization of this chat experience in the form of questions and answers (Q&amp;A). When enterprise build apps using LLMs, they are likely to use a combination of chat and Q&amp;A experiences. These apps are more useful when they are able to integrate existing apps and services activated based on the context of the chat or Q&amp;A experience. Think of this as a conversation with your assistant about a certain topic. At some point you may suggest actions for the assistant to execute based on the context of the conversation. This is where the concept of functions come in. Functions are a way to specify actions that can be executed based on the context of the conversation.</p> <p>OpenAI Functions were introduced in July 2023 as a way for developers to have GPT 4 and GPT 3.5 models identify when to process user inputs and extract JSON arguments matching function specs specified in the chat context.</p>","tags":["Functions","OpenAI","Guidance"]},{"location":"functions/openai-functions/#guidance-for-using-openai-functions","title":"Guidance for using OpenAI Functions","text":"<p>This section is a collection of tips based on our experience using OpenAI Functions.</p>","tags":["Functions","OpenAI","Guidance"]},{"location":"functions/openai-functions/#simple-project-structure","title":"Simple project structure","text":"<p>GenAPI project structure recommends writing the function spec next to the function definition so that it is easy to keep both in sync with any future changes.</p>","tags":["Functions","OpenAI","Guidance"]},{"location":"functions/openai-functions/#system-message","title":"System message","text":"<p>Use system prompt or meta prompt to instruct GPT model to (a) ask user for arguments instead of hallicunating or making assumptions, (b) call function only once per conversation step, \u00a9 control how the LLM treats function response, and (d) control how the LLM responds.</p> System prompt to instruct GPT model<pre><code>messages.append(\n    {\n        \"role\": \"system\", \n        \"content\": '''Ask the user for arguments when calling a function. \n        Respond after understanding function response and user intent, if they expect,\n        function response only or\n        function response and your response.\n        Don't call the same function more than once for each user prompt.\n        Remain crisp and to the point in your responses.'''\n    })\n</code></pre>","tags":["Functions","OpenAI","Guidance"]},{"location":"functions/openai-functions/#simple-specifications","title":"Simple specifications","text":"<p>You can specify a function spec in a similar format as a typical function documentation describing the function name, description, arguments, and return values. Start with minimal description, provide an example where applicable, test the function, and add more details as needed.</p> Weather function specification<pre><code>weather_spec = {\n    \"name\": \"weather\",\n    \"description\": \"Get the current weather\",\n    \"parameters\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"city\": {\n                \"type\": \"string\",\n                \"description\": \"The city, e.g. San Francisco\",\n            },\n            \"state\": {\n                \"type\": \"string\",\n                \"description\": \"The state code, e.g. CA\",\n            },\n            \"country\": {\n                \"type\": \"string\",\n                \"description\": \"The country, e.g. USA\",\n            },\n            \"units\": {\n                \"type\": \"string\", \n                \"enum\": [\"metric\", \"imperial\"],\n                \"description\": \"Units to use when reporting weather data.\",\n            },\n        },\n        \"required\": [\"city\", \"state\", \"country\"],\n    },\n}\n</code></pre>","tags":["Functions","OpenAI","Guidance"]},{"location":"functions/openai-functions/#function-definition-guidance","title":"Function definition guidance","text":"<p>Let's note some important points which help in integrating the function with an LLM. First, the function takes in arguments in the same sequence as in the specification. Second, the required arguments match the speficication and any arguments that are not required have a default value set. Third, when calling external APIs, the function is able to gracefully handle errors and return a JSON response with an error message. Fourth, the function returns a JSON response with keys identifiable in plain English by the LLM. Fifth, the return value JSON object keys are not defined in the specification. LLM is able to convert this JSON response into natural language response without the need for a specification.</p> Weather function code<pre><code>def weather(city, state, country, units=\"imperial\"):\n    api_key = os.getenv(\"OPEN_WEATHER_MAP_KEY\")\n    base_url = \"http://api.openweathermap.org/data/2.5/weather\"\n    complete_api_link = f\"{base_url}?q={city},{state},{country}&amp;units={units}&amp;appid={api_key}\"\n\n    response = requests.get(complete_api_link)\n\n    if response.status_code != 200:\n        return json.dumps({\"error\": \"Unable to fetch weather data\"})\n\n    data = response.json()\n\n    weather_info = {\n        \"location\": f\"{city}, {state}, {country}\",\n        \"temperature\": data['main']['temp'],\n        \"units\": 'Celsius' if units == 'metric' else 'Fahrenheit',\n        \"forecast\": data['weather'][0]['description'],\n    }\n    return json.dumps(weather_info)\n</code></pre>","tags":["Functions","OpenAI","Guidance"]},{"location":"functions/openai-functions/#techniques-for-using-functions-with-llms","title":"Techniques for using functions with LLMs","text":"","tags":["Functions","OpenAI","Guidance"]},{"location":"functions/openai-functions/#llm-response-based-on-function-response","title":"LLM response based on function response","text":"<p>Most common use of functions is where you want the LLM response to be based on the function response. For example, you may want to ask the user for a city and state and then call the weather function to get the weather for the specified location. You may then want to respond to the user with the weather information. In this case, you can use the function response as the LLM response. See example in the cookbook OpenAI Functions with Climate APIs for more details.</p>","tags":["Functions","OpenAI","Guidance"]},{"location":"functions/openai-functions/#combining-multiple-function-calls-in-a-single-response","title":"Combining multiple function calls in a single response","text":"<p>You may want to combine multiple function calls in a single response. For example, you may want to ask the user for a city and state and then call the weather and air quality function. You may then want to respond to the user with the weather and air quality information. In this case, you can use multiple function responses as the LLM response. GenAPI enables combining two or more functions in same response. See example in the cookbook OpenAI Functions with Climate APIs for more details.</p>","tags":["Functions","OpenAI","Guidance"]},{"location":"functions/openai-functions/#function-response-overriding-llm-response","title":"Function response overriding LLM response","text":"<p>You may want to override the LLM response with the function response. For example, user may want to render a painting with specific arguments. The LLM may also be able to generate an image however you may prefer to use the function response to create a custom painting based on user arguments. In this case, you can use the function response to override the LLM response. See example in the cookbook OpenAI Functions with Render APIs for more details.</p>","tags":["Functions","OpenAI","Guidance"]},{"location":"functions/openai-functions/#combining-function-response-and-llm-response","title":"Combining function response and LLM response","text":"<p>Sometimes you want LLM to generate certain response, like a Markdown table of top 5 tallest buildings in the world. Then you may want to call a function to genrate a chart based on this table. Then you are interested in combining function response and LLM response when responding to the user. See example in the cookbook OpenAI Functions with Render APIs for more details.</p>","tags":["Functions","OpenAI","Guidance"]},{"location":"functions/openai-functions/#reusable-functions-library","title":"Reusable functions library","text":"","tags":["Functions","OpenAI","Guidance"]},{"location":"functions/openai-functions/#weather","title":"Weather","text":"<p>Weather function is a simple function that takes in city, state, and country as arguments and returns the current weather for the specified location. The function uses OpenWeatherMap API to fetch the weather data. The function returns a JSON response with the following keys: location, temperature, units, and forecast. The function is available in the functions library on the GenAPI GitHub.</p>","tags":["Functions","OpenAI","Guidance"]},{"location":"functions/openai-functions/#air-quality","title":"Air Quality","text":"<p>Air quality function is a simple function that takes in city, state, and country as arguments and returns the current air quality for the specified location. The function uses IQAir Air Visual API to fetch the air quality data. The function returns a JSON response with the following keys: location and air quality index. The function is available in the functions library on the GenAPI GitHub.</p>","tags":["Functions","OpenAI","Guidance"]},{"location":"functions/openai-functions/#molecule-inventor","title":"Molecule Inventor","text":"<p>We introduce a new GenAPI render function for visualizing molecules using SMILES string. Simplified molecular-input line-entry system or SMILES is a specification in the form of a line notation for describing the structure of chemical species using short ASCII strings. SMILES strings can be imported by most molecule editors for conversion back into two-dimensional drawings or three-dimensional models of the molecules.</p> <p>This function requires GPT-4 to work as GPT-3.5 does not generate SMILES strings. Let us start by writing the function specification and definition. The function is available in the functions library on the GenAPI GitHub.</p>","tags":["Functions","OpenAI","Guidance"]},{"location":"functions/openai-functions/#painting-generator","title":"Painting generator","text":"<p>Painting function is used to generate a painting based on given user inputs on painting subject, background, etc. We tried to design our API will a lot of flexibility in terms of the input parameters. At the same time our required arguments are minimal to keep prompt variations from simple to more involved. The function is available in the functions library on the GenAPI GitHub.</p>","tags":["Functions","OpenAI","Guidance"]},{"location":"functions/openai-functions/#chart-generator","title":"Chart generator","text":"<p>Chart function is used to generate a chart based on the LLM response as a Markdown table. We can choose to render a bar, line, or point chart. The function is available in the functions library on the GenAPI GitHub.</p>","tags":["Functions","OpenAI","Guidance"]},{"location":"transformers/huggingface-gen-ai-dev-setup-on-mac/","title":"Hugging Face Generative AI Developer Setup on a Mac","text":"<p>This article explains step-by-step how to setup your Mac for developing Generative AI applications using Hugging Face Transformer models. We will use Jupyter Notebook for our development environment. We will install the minimum set of dependencies required to get started developing using deep learning transformer models available at Hugging Face. Let us first understand the reasons why you should consider a Mac for your Gen AI development over a Cloud based setup using the most popular NVIDIA GPU options.</p>","tags":["Generative AI","Hugging Face"]},{"location":"transformers/huggingface-gen-ai-dev-setup-on-mac/#performance-vs-cost-vs-availability","title":"Performance vs Cost vs Availability","text":"<p>GPUs are the hardest to source components of your Gen AI stack. They are also the most expensive decisions you will make when building your Gen AI apps.</p> <p>As of Nov 2023, Apple has announced the MacBook Pro with M3 Max chip featuring 16-core CPU with 12 performance cores and 4 efficiency cores, 40-core GPU, Hardware-accelerated ray tracing, 16-core Neural Engine, and 400GB/s memory bandwidth. It has 48GB unified memory which is configurable to 128GB. The storage of 1TB SSD is configurable to 8TB.</p> <p></p> <p>Surprisingly, on performance M3 Max comes pretty close when you consider TFLOPS (number of floating point calculations per second), number of transistors packed into the chip, GPU accessible memory and bandwidth. Integrated SSD storage means when compared to cloud based H100 setups, you win on network latency and bandwidth.</p> <p>You can buy a Mac laptop relatively easily starting as low as $3,200 one time cost. An Nvidia H100 PCIe is around 10x that price at $32,000 and that is only the chip. You will need a cloud provider like AWS supporting H100s if you want to use these. As of Nov 2023, when you search for AWS P5 instances (available with 8 to 48 H100 configurations), you may only find p5.48xlarge instances available at around $80,000 monthly cost.</p>","tags":["Generative AI","Hugging Face"]},{"location":"transformers/huggingface-gen-ai-dev-setup-on-mac/#training-vs-inference","title":"Training vs Inference","text":"<p>Training an LLM is significantly more GPU intensive depending on several factors including the size of the model, the size of the dataset, the number of epochs, and the batch size. Inference is less GPU intensive as you are not training the model but using the model to generate text or images. There have been significant advancements in making inference more efficient, such as model quantization, pruning, and the use of dedicated inference chips. These techniques can reduce the computational load and the number of GPUs needed for running a trained model.</p> <p>Hugging Face offers many pretrained transformer models which you can use for inference. You can also use these models for transfer learning. This means you can use a pretrained model and fine tune it for your specific use case. This is a great way to get started with Gen AI development without having to train your own models.</p> <p>Note</p> <p>At the time of writing this article we are using a 2022 MacBook Pro 2 GHz Quad-Core Intel Core i5, Intel Iris Plus Graphics 1536 MB, and 16GB RAM, which costs around $1,700. As we will mostly use pretrained models for inference, we will not even need a GPU for our development setup!</p>","tags":["Generative AI","Hugging Face"]},{"location":"transformers/huggingface-gen-ai-dev-setup-on-mac/#open-source-innovation","title":"Open Source Innovation","text":"<p>As on Nov 2023, Hugging Face lists more than 400,000 models and more than 79,000 datasets which can be downloaded on your Mac for learning transfer models or using these for building personal AI projects.</p> <p>Open source models are usually ahead of close source models by a few months when it comes to new research. Hugging Face had Visual Question Answering model available several months sooner than ChatGPT and Bard support for visual imputs. Hugging Face also has a large community of developers contributing to the open source models and datasets. This means you can get help from the community when you are stuck with a problem.</p>","tags":["Generative AI","Hugging Face"]},{"location":"transformers/huggingface-gen-ai-dev-setup-on-mac/#personal-ai","title":"Personal AI","text":"<p>There may be cases where for company confidentiality reasons, to stay ahead of peer competition, or for self learning, you may want to use your own laptop based Gen AI setup. A Mac setup can also serve as local developer environment for your cloud based Gen AI app. Some of the best practices of optimizing for resource contrained devices like a laptop can also help save dollars on cloud based Gen AI apps.</p> <p>With a business case for personal Gen AI established, let us get started with our Mac based development setup.</p>","tags":["Generative AI","Hugging Face"]},{"location":"transformers/huggingface-gen-ai-dev-setup-on-mac/#setup-development-environment","title":"Setup Development Environment","text":"<p>First, you should be running the latest Python on your system with Python package manager upgraded to the latest.</p> <pre><code>python --version\n# should return Python 3.10.x or higher as on Jan'23\npip --version\n# should return pip 22.3.x or higher as on Jan'23\n</code></pre> <p>Follow this guide for Mac OS X if you do not have the latest Python. If installing specific version of Python for managing dependencies then follow this thread to install using <code>pyenv</code> Python version manager. If required upgrade pip to the latest using the following command.</p> <pre><code>pip install --user --upgrade pip\n</code></pre> <p>We will now create a virtual environment for our setup so that our dependencies are isolated and do not conflict with the system installed packages. We will follow this guide for creating and managing the virtual environment. First change to the directory where we will develop our application.</p> <pre><code>python -m venv env\n</code></pre> <p>If you run ls env you will see following folders and files created.</p> <pre><code>bin        include    lib        pyvenv.cfg\n</code></pre> <p>Now we can activate our virtual environment like so. You will notice that development directory prefixed with the (env) to indicate you are now running in the virtual environment.</p> <pre><code>. env/bin/activate\n</code></pre> <p>You can confirm that you are not running inside the virtual environment with its own Python.</p> <pre><code>which python\n## should return /Users/.../env/bin/python\n</code></pre> <p>To leave the virtual environment using the <code>deactivate</code> command. Re-enter using same command as earlier.</p> <p>Now we are ready to install our dependencies for running Hugging Face Transformer models with PyTorch.</p> <pre><code>pip install torch torchvision transformers\n</code></pre> <p>We can test our installation with the following script.</p> Test Installation<pre><code>python -c \"from transformers import pipeline; \\\nprint(pipeline('sentiment-analysis')('we love you'))\"\n\n# [{'label': 'POSITIVE', 'score': 0.9998704195022583}]\n</code></pre> <p>Now we can setup our development environment which is Jupyter Notebook and Jupyter Widgets.</p> <pre><code>pip install notebook ipywidgets\n</code></pre> <p>One last thing we will do is to setup a custom cache directory for our models and datasets downloaded when using Hugging Face. Edit your bash script. On zsh shell this is the  ~/.zshrc file. This is required as the default cache directory is in the home directory which is not a good idea as it will fill up your home directory with large files. When you run Hugging Face API for the first time, it will download the models and datasets to the default cache directory. You can change the default cache directory by setting the environment variables.</p> ~/.zshrc<pre><code>export TRANSFORMERS_CACHE=\"/Users/.../cache/transformers\"\nexport HUGGINGFACE_HUB_CACHE=\"/Users/.../cache/hub\"\nexport HF_HOME=\"/Users/.../cache/huggingface\"\n</code></pre> <p>That is it! We setup a development environment with Jupyter Notebook. We installed the minimum set of dependencies required to get started developing using pretrained transformer models available at Hugging Face.</p>","tags":["Generative AI","Hugging Face"]},{"location":"transformers/huggingface-gen-ai-dev-setup-on-mac/#explore-hugging-face-models","title":"Explore Hugging Face Models","text":"<p>Our next step is to explore the Hugging Face models by inference tasks. We will use the Hugging Face pipelines for this purpose.</p> <p>Hugging Face offers more than 400,000 models which are classified under six categories of around 40 inference tasks - multimodal, computer vision, natural language processing, audio, tabular, and reinforcement learning. An example of a multimodal inference task is text to image generation. Likewise, an example of a computer vision inference task is image classification. When exploring Hugging Face models, unless you know exactly which model you want to use, it is best to start with the inference task and then explore the models available for that task.</p> <p></p>","tags":["Generative AI","Hugging Face"]},{"location":"transformers/huggingface-gen-ai-dev-setup-on-mac/#hugging-face-pipelines","title":"Hugging Face Pipelines","text":"<p>Let us now fire up the <code>juputer notebook</code> using Terminal and enter our first pipeline to explore Automatic Speech Recognition (ASR) models. We will take the famous statement by Martin Luther King Jr. and convert it to text using the ASR model.</p> Explore Hugging Face pipelines<pre><code>from transformers import pipeline\nmodel = pipeline(task=\"automatic-speech-recognition\")\nmodel(\"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac\")\n\n# {'text': 'I HAVE A DREAM BUT ONE DAY THIS NATION \n# WILL RISE UP LIVE UP THE TRUE MEANING OF ITS TREES'}\n</code></pre> <p>As you execute the pipeline statement, you will notice following message in the notebook cell output. This means the pipeline API automatically selects the best model for the task. In this case, it is the wav2vec2-base-960h model.</p> Hugging Face pipeline output<pre><code>No model was supplied, defaulted to facebook/wav2vec2-base-960h \nand revision 55bb623 (https://huggingface.co/facebook/wav2vec2-base-960h).\nUsing a pipeline without specifying a model name \nand revision in production is not recommended.\n</code></pre> <p>You can also check the <code>model_cache/transformers</code> folder to see the downloaded model files under a folder named after the model name and revision <code>models--facebook--wav2vec2-base-960h</code> in this case. A quick Get Info on the folder will show you the size of the model files. In this case, it is 392MB.</p> <p>Next you will notice the inference output is not entirely accurate. The accurate statement is <code>I have a dream that one day this nation will rise up and live out the true meaning of its creed</code> as per NPR source. However, note that actual speech does not have the <code>and</code> word in the phrase <code>rise up, live out</code>. This is an important note for us to remember when we try to improve the accuracy of our Gen AI app.</p>","tags":["Generative AI","Hugging Face"]},{"location":"transformers/huggingface-gen-ai-dev-setup-on-mac/#model-hub","title":"Model Hub","text":"<p>You can visit the model hub filtered by inference task (https://huggingface.co/models?pipeline_tag=automatic-speech-recognition) to see if there are more trending models than the one used by pipelines API. Higher trending models are more recently updated and have more downloads and votes. Higher trending models are usually more accurate.</p> <p></p> <p></p>","tags":["Generative AI","Hugging Face"]},{"location":"transformers/huggingface-gen-ai-dev-setup-on-mac/#model-hub-and-card-workflow","title":"Model Hub and Card Workflow","text":"<p>As you are optimizing for memory, inference time, and accuracy, you would want to do some research before trying out a trending model and see if it improves the inference accuracy, yet does not cost compute and memory. The top trending model may be most accurate, however it may also be the most expensive on resources.</p> <p></p> <p>You may intuitively start from the top of the model hub list and read the model card for openai/whisper-large-v3. You can check the model size by switching to Files and versions tab and notice the <code>pytorch_model.bin</code> pickle file is 3GB which is almost 10x the size of the wav2vec2-base-960h model. The whisper-v3 model is multilingual however our use case only requires English support. So we read the model card and check whisper-tiny.en which is only 151MB, half the size of the wav2vec2-base-960h model. Nice! Let us try this model.</p> Load model directly<pre><code>model = pipeline(model=\"openai/whisper-tiny.en\")\nmodel(\"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac\")\n\n# {'text': ' I have a dream that one day this nation \n# will rise up live out the true meaning of its creed'}\n</code></pre> <p>This is 100% accurate result with 60% smaller, top trending model, and faster inference.</p> <p>Let us be greedy and see if we can do even better than the input speech and add proper English grammar. Notice that our output missed <code>and</code> in the sentence. Let us try the next larger model https://huggingface.co/openai/whisper-base.en which is 290MB in size and still smaller than the wav2vec2-base-960h model.</p>","tags":["Generative AI","Hugging Face"]},{"location":"transformers/huggingface-gen-ai-dev-setup-on-mac/#exploring-and-dowlnoading-datasets-locally","title":"Exploring and dowlnoading datasets locally","text":"<p>Before we download the model we can try it out using Hugging Face Inference API. Let us first download the speech file locally by visiting Hugging Face Datasets and searching for <code>Narsil</code> to find the dataset card for https://huggingface.co/datasets/Narsil/asr_dummy. We can now switch to Files and versions tab to download the <code>mlk.flac</code> file locally.</p>","tags":["Generative AI","Hugging Face"]},{"location":"transformers/huggingface-gen-ai-dev-setup-on-mac/#evaluating-the-model-using-inference-api","title":"Evaluating the model using Inference API","text":"<p>Now let us head back to the https://huggingface.co/openai/whisper-base.en model card and use the Inference API to test the model.</p> <p></p> <p>Note the computation time message below the Compute button. This way we can also quickly compare the inference time for different models before we cache these locally.</p> <p>This is the result we are looking for. Let us cache the model locally and use it in our pipeline.</p> <p>Improve app accuracy incrmentally<pre><code>model = pipeline(model=\"openai/whisper-base.en\")\nmodel(\"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac\")\n\n# {'text': ' I have a dream that one day this nation\n# will rise up and live out the true meaning of its creed.'}\n</code></pre> Perfect! We have improved our Gen AI app accuracy incrementally by using a trending model. </p>","tags":["Generative AI","Hugging Face"]},{"location":"transformers/huggingface-gen-ai-dev-setup-on-mac/#calculating-inference-time","title":"Calculating inference time","text":"<p>Now we can add <code>%time</code> magic command to the cells in our Jupyter Notebook where we call the <code>model</code> function. Here are the results on our MacBook Pro 2 GHz Quad-Core Intel Core i5 CPU, Intel Iris Plus Graphics 1.5GB integrated GPU, and 16GB RAM.</p> Inference time<pre><code># facebook/wav2vec2-base-960h (392MB cache size)\nCPU times: user 18 \u00b5s, sys: 1 \u00b5s, total: 19 \u00b5s\nWall time: 7.87 \u00b5s\n\n# openai/whisper-tiny.en (151MB cache size)\nCPU times: user 2 \u00b5s, sys: 0 ns, total: 2 \u00b5s\nWall time: 5.96 \u00b5s\n\n# openai/whisper-base.en (290MB cache size)\nCPU times: user 2 \u00b5s, sys: 0 ns, total: 2 \u00b5s\nWall time: 4.77 \u00b5s\n</code></pre> <p>Note that the facebook model uses multiple CPU cores to complete the task (wall time) sooner than CPU time (total time spent by multiple cores). Whereas whisper-tiny.en and whisper-base do not seem to benefit from multi-core processing. Yet we save 40% in inference time with openai over facebook models. This adds up when you are running inference on a large dataset.</p> <p>We have demonstrated multimodal capabilities of latest OpenAI Whisper v3 model which not only accurately recognized speech to text but also used text to text grammar capabilities to improve the inference output. We have also selected optimal model for our use case with reduced size by 27% and improved the inference time. We can now use this model for our app.</p>","tags":["Generative AI","Hugging Face"]},{"location":"tags/","title":"Tags","text":""},{"location":"tags/#dalle","title":"DALL.E","text":"<ul> <li>OpenAI Functions with Render APIs</li> </ul>"},{"location":"tags/#functions","title":"Functions","text":"<ul> <li>Molecule Inventor with GPT-4 Functions</li> <li>OpenAI Functions with Climate API</li> <li>OpenAI Functions with Render APIs</li> <li>OpenAI Functions</li> </ul>"},{"location":"tags/#gpt-4","title":"GPT-4","text":"<ul> <li>Molecule Inventor with GPT-4 Functions</li> </ul>"},{"location":"tags/#generative-ai","title":"Generative AI","text":"<ul> <li>Hugging Face Gen AI Dev Setup on Mac</li> </ul>"},{"location":"tags/#guidance","title":"Guidance","text":"<ul> <li>OpenAI Functions</li> </ul>"},{"location":"tags/#hugging-face","title":"Hugging Face","text":"<ul> <li>Hugging Face Gen AI Dev Setup on Mac</li> </ul>"},{"location":"tags/#image-generation","title":"Image Generation","text":"<ul> <li>OpenAI Functions with Render APIs</li> </ul>"},{"location":"tags/#openai","title":"OpenAI","text":"<ul> <li>OpenAI Functions with Climate API</li> <li>OpenAI Functions with Render APIs</li> <li>OpenAI Functions</li> </ul>"}]}